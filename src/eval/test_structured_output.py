#!/usr/bin/env python3
"""
æµ‹è¯•ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½çš„ç®€å•è„šæœ¬
"""

import json
from item_eval import (
    create_dimension_model, 
    PsychologicalItemEvaluator,
    CostConfig
)
from langchain_core.output_parsers import JsonOutputParser

def test_dimension_model_creation():
    """æµ‹è¯•åŠ¨æ€ç»´åº¦æ¨¡å‹åˆ›å»º"""
    print("ğŸ§ª æµ‹è¯•åŠ¨æ€ç»´åº¦æ¨¡å‹åˆ›å»º...")
    
    # å®šä¹‰æµ‹è¯•ç»´åº¦
    dimensions = [
        {
            "name": "TestDimension1",
            "description": "è¿™æ˜¯æµ‹è¯•ç»´åº¦1"
        },
        {
            "name": "TestDimension2", 
            "description": "è¿™æ˜¯æµ‹è¯•ç»´åº¦2"
        }
    ]
    
    try:
        # åˆ›å»ºåŠ¨æ€æ¨¡å‹
        DynamicModel = create_dimension_model(dimensions)
        
        # åˆ›å»ºè§£æå™¨
        parser = JsonOutputParser(pydantic_object=DynamicModel)
        
        # è·å–æ ¼å¼æŒ‡ä»¤
        format_instructions = parser.get_format_instructions()
        print("âœ… æˆåŠŸåˆ›å»ºåŠ¨æ€æ¨¡å‹")
        print("ğŸ“‹ æ ¼å¼æŒ‡ä»¤:")
        print(format_instructions)
        
        # æµ‹è¯•æœ‰æ•ˆæ•°æ®
        valid_data = {"TestDimension1": "A", "TestDimension2": "B"}
        instance = DynamicModel(**valid_data)
        print(f"âœ… æœ‰æ•ˆæ•°æ®æµ‹è¯•é€šè¿‡: {instance.dict()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŠ¨æ€æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False

def test_structured_output_parsing():
    """æµ‹è¯•ç»“æ„åŒ–è¾“å‡ºè§£æ"""
    print("\nğŸ§ª æµ‹è¯•ç»“æ„åŒ–è¾“å‡ºè§£æ...")
    
    dimensions = [
        {
            "name": "Quality",
            "description": "é¢˜ç›®è´¨é‡è¯„ä¼°"
        },
        {
            "name": "Clarity", 
            "description": "é¢˜ç›®æ¸…æ™°åº¦è¯„ä¼°"
        }
    ]
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹ï¼ˆä¸éœ€è¦çœŸå®çš„APIå¯†é’¥æ¥æµ‹è¯•æ¨¡å‹åˆ›å»ºï¼‰
        import os
        os.environ["OPENAI_API_KEY"] = "test-key-for-model-creation"
        
        evaluator = PsychologicalItemEvaluator(
            cost_config=CostConfig(input_token_rate=0.0, output_token_rate=0.0)
        )
        
        # è®¾ç½®ç»“æ„åŒ–è¾“å‡º
        evaluator.setup_structured_output(dimensions)
        
        print("âœ… ç»“æ„åŒ–è¾“å‡ºè§£æå™¨è®¾ç½®æˆåŠŸ")
        print(f"ğŸ“‹ è§£æå™¨ç±»å‹: {type(evaluator.json_parser)}")
        print(f"ğŸ“‹ æ¨¡å‹å­—æ®µ: {list(evaluator.dimension_model.__fields__.keys())}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç»“æ„åŒ–è¾“å‡ºè§£æå™¨è®¾ç½®å¤±è´¥: {e}")
        return False

def test_fallback_parsing():
    """æµ‹è¯•å›é€€è§£æåŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•å›é€€è§£æåŠŸèƒ½...")
    
    dimensions = [
        {"name": "TestDim1", "description": "æµ‹è¯•ç»´åº¦1"},
        {"name": "TestDim2", "description": "æµ‹è¯•ç»´åº¦2"}
    ]
    
    try:
        import os
        os.environ["OPENAI_API_KEY"] = "test-key"
        
        evaluator = PsychologicalItemEvaluator()
        
        # æµ‹è¯•æœ‰æ•ˆJSON
        valid_json = '{"TestDim1": "A", "TestDim2": "B"}'
        result = evaluator._parse_multi_dimension_evaluation_response_fallback(
            valid_json, dimensions
        )
        print(f"âœ… æœ‰æ•ˆJSONè§£æ: {result}")
        
        # æµ‹è¯•æ— æ•ˆJSON
        invalid_json = '{"TestDim1": "A", "TestDim2": "C"}'  # Cæ˜¯æ— æ•ˆå€¼
        result = evaluator._parse_multi_dimension_evaluation_response_fallback(
            invalid_json, dimensions
        )
        print(f"âœ… éƒ¨åˆ†æ— æ•ˆJSONè§£æ: {result}")
        
        # æµ‹è¯•å®Œå…¨æ— æ•ˆJSON
        broken_json = 'not a json at all'
        result = evaluator._parse_multi_dimension_evaluation_response_fallback(
            broken_json, dimensions
        )
        print(f"âœ… æ— æ•ˆJSONè§£æ: {result}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å›é€€è§£ææµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½æµ‹è¯•\n")
    
    tests = [
        test_dimension_model_creation,
        test_structured_output_parsing,
        test_fallback_parsing
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")