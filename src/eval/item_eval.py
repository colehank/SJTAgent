import asyncio
import json
import pandas as pd
from itertools import combinations
from typing import Dict, List, Tuple, Any, Optional, Annotated
from dataclasses import dataclass, asdict
from datetime import datetime

import os
from tqdm import tqdm
import operator
import nest_asyncio
import tiktoken

from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field, create_model
from typing_extensions import TypedDict
from dotenv import load_dotenv
load_dotenv()

@dataclass
class PairwiseEvaluation:
    """é…å¯¹è¯„ä¼°ç»“æœæ•°æ®ç»“æ„"""
    item1_id: str
    item2_id: str
    dimension: str
    winner: str
    evaluation_time: str

@dataclass
class TokenUsage:
    """Tokenä½¿ç”¨ç»Ÿè®¡"""
    input_tokens: int = 0
    output_tokens: int = 0
    total_tokens: int = 0
    
    def add(self, input_tokens: int, output_tokens: int):
        """æ·»åŠ tokenä½¿ç”¨é‡"""
        self.input_tokens += input_tokens
        self.output_tokens += output_tokens
        self.total_tokens += (input_tokens + output_tokens)

@dataclass 
class CostConfig:
    """è´¹ç”¨é…ç½®"""
    input_token_rate: float  # è¾“å…¥tokenè´¹ç‡ (per 1M tokens)
    output_token_rate: float  # è¾“å‡ºtokenè´¹ç‡ (per 1M tokens)
    
    def calculate_cost(self, token_usage: TokenUsage) -> float:
        """è®¡ç®—æ€»è´¹ç”¨"""
        input_cost = (token_usage.input_tokens / 1_000_000) * self.input_token_rate
        output_cost = (token_usage.output_tokens / 1_000_000) * self.output_token_rate
        return input_cost + output_cost

class DimensionEvaluation(BaseModel):
    """å•ä¸ªç»´åº¦è¯„ä¼°ç»“æœçš„Pydanticæ¨¡å‹ï¼Œç”¨äºç»“æ„åŒ–è¾“å‡º"""
    class Config:
        extra = "forbid"  # ç¦æ­¢é¢å¤–å­—æ®µ
    
    def __init__(self, **data):
        # åŠ¨æ€æ·»åŠ ç»´åº¦å­—æ®µ
        for key, value in data.items():
            if key not in self.__fields__:
                self.__fields__[key] = Field(..., description=f"Evaluation result for {key} dimension")
        super().__init__(**data)

def create_dimension_model(dimensions: list[dict[str, str]]) -> type:
    """åŠ¨æ€åˆ›å»ºåŒ…å«æ‰€æœ‰ç»´åº¦çš„Pydanticæ¨¡å‹"""
    from pydantic import create_model
    
    # ç®€åŒ–å­—æ®µå®šä¹‰ï¼Œå»æ‰å¤æ‚çš„é…ç½®
    fields = {}
    for dim in dimensions:
        field_name = dim['name']
        fields[field_name] = (str, Field(..., description=f"Choose 'A' or 'B' for {dim['description']}"))
    
    # ä½¿ç”¨ pydantic.create_model åŠ¨æ€åˆ›å»ºæ¨¡å‹
    DynamicDimensionModel = create_model('DynamicDimensionModel', **fields)
    
    return DynamicDimensionModel

class EvaluationState(TypedDict):
    """LangGraphçŠ¶æ€å®šä¹‰ - ä½¿ç”¨TypedDictå’Œreducers"""
    test_items: dict[str, dict[str, Any]]
    dimensions: list[dict[str, str]]
    
    # å¤„ç†è¿‡ç¨‹æ•°æ® - ä½¿ç”¨reducersé¿å…å¹¶å‘æ›´æ–°å†²çª
    pairs_to_evaluate: Annotated[list[tuple[str, str, str]], operator.add]
    completed_evaluations: Annotated[list[PairwiseEvaluation], operator.add]
    current_batch: list[tuple[str, str, str]]
    batch_results: Annotated[list[PairwiseEvaluation], operator.add]
    
    # è¾“å‡ºæ•°æ®
    final_results: Optional[pd.DataFrame]
    
    # é…ç½®
    batch_size: int
    max_concurrent: int
    cost_config: Optional[CostConfig]
    
    # Tokenç»Ÿè®¡
    token_usage: TokenUsage
    
    # è¿›åº¦æ¡ç›¸å…³
    total_pairs: int
    progress_bar: Optional[tqdm]
    show_progress: bool


class PsychologicalItemEvaluator:
    """å¿ƒç†æµ‹éªŒé¢˜ç›®è¯„ä¼°å™¨"""
    
    def __init__(
        self, 
        model_name: str = "qwen-plus", 
        temperature: float = 0.3, 
        api_key: Optional[str] = None, 
        cost_config: Optional[CostConfig] = None,
        sys_prompt: str = "ä½ æ˜¯ä¸€ä½å¿ƒç†æµ‹éªŒä¸“å®¶ï¼Œè´Ÿè´£è¯„ä¼°é¢˜ç›®è´¨é‡ã€‚"
    ) -> None:
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key
        elif not os.getenv("OPENAI_API_KEY"):
            raise ValueError("è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡æˆ–åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥api_keyå‚æ•°")
        
        self.sys_prompt = sys_prompt
        
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=temperature
        )
        
        self.cost_config = cost_config or CostConfig(
            input_token_rate=0.8,
            output_token_rate=2.0
        )
        
        # åˆå§‹åŒ–JSONè§£æå™¨ï¼ˆå°†åœ¨è¯„ä¼°æ—¶åŠ¨æ€åˆ›å»ºï¼‰
        self.json_parser = None
        self.dimension_model = None
        
        try:
            self.tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")
        except KeyError:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")

    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        return len(self.tokenizer.encode(text))
    
    def setup_structured_output(self, dimensions: list[dict[str, str]]) -> None:
        """è®¾ç½®ç»“æ„åŒ–è¾“å‡ºè§£æå™¨"""
        # åˆ›å»ºåŠ¨æ€ç»´åº¦æ¨¡å‹
        self.dimension_model = create_dimension_model(dimensions)
        # åˆ›å»ºJSONè§£æå™¨
        self.json_parser = JsonOutputParser(pydantic_object=self.dimension_model)
    
    def estimate_cost_for_evaluation(
        self,
        test_items: dict[str, dict[str, Any]],
        dimensions: list[dict[str, str]]
    ) -> tuple[TokenUsage, float]:
        """é¢„ä¼°è¯„ä¼°çš„tokenä½¿ç”¨é‡å’Œè´¹ç”¨"""
        
        # è®¡ç®—é…å¯¹æ•°
        total_pairs = len(list(combinations(test_items.keys(), 2)))
        
        # åˆ›å»ºæ ·æœ¬æç¤ºè¯æ¥ä¼°ç®—tokenä½¿ç”¨
        sample_items = list(test_items.items())[:2] if len(test_items) >= 2 else list(test_items.items())
        
        if len(sample_items) < 2:
            # å¦‚æœé¢˜ç›®ä¸è¶³2ä¸ªï¼Œå¤åˆ¶ç¬¬ä¸€ä¸ªé¢˜ç›®ä½œä¸ºæ ·æœ¬
            sample_items = [sample_items[0], sample_items[0]]
        
        # print(sample_items)
        sample_prompt = self.create_single_eval(
            sample_items[0][1], sample_items[1][1], 
            dimensions
        )
        
        # è®¡ç®—å•æ¬¡è°ƒç”¨çš„tokenä½¿ç”¨
        input_tokens_per_call = self.count_tokens(self.sys_prompt) + self.count_tokens(sample_prompt)
        
        # ä¼°ç®—è¾“å‡ºtokenï¼ˆåŸºäºç»´åº¦æ•°é‡å’Œç»éªŒå€¼ï¼‰
        estimated_output_per_dimension = 10  # æ¯ä¸ªç»´åº¦çº¦è¾“å‡º10ä¸ªtoken
        output_tokens_per_call = len(dimensions) * estimated_output_per_dimension + 20  # é¢å¤–20ä¸ªtokenç”¨äºJSONæ ¼å¼
        
        # è®¡ç®—æ€»çš„tokenä½¿ç”¨é‡
        total_input_tokens = input_tokens_per_call * total_pairs
        total_output_tokens = output_tokens_per_call * total_pairs
        
        estimated_usage = TokenUsage(
            input_tokens=total_input_tokens,
            output_tokens=total_output_tokens,
            total_tokens=total_input_tokens + total_output_tokens
        )
        
        estimated_cost = self.cost_config.calculate_cost(estimated_usage)
        
        return estimated_usage, estimated_cost
    
    def create_evaluation_workflow(self) -> StateGraph:
        """åˆ›å»ºLangGraphå·¥ä½œæµ"""
        workflow = StateGraph(EvaluationState)  # ä½¿ç”¨å®šä¹‰çš„EvaluationStateç±»
        
        workflow.add_node("generate_pairs", self.generate_pairs)
        workflow.add_node("batch_evaluations", self.batch_evaluations)
        workflow.add_node("process_batch", self.process_batch)
        workflow.add_node("aggregate_results", self.aggregate_results)
        workflow.add_node("create_dataframe", self.create_dataframe)
        
        workflow.set_entry_point("generate_pairs")
        workflow.add_edge("generate_pairs", "batch_evaluations")
        workflow.add_conditional_edges(
            "batch_evaluations",
            self.should_continue_batching,
            {
                "continue": "process_batch",
                "end": "aggregate_results"
            }
        )
        workflow.add_edge("process_batch", "batch_evaluations")
        workflow.add_edge("aggregate_results", "create_dataframe")
        workflow.add_edge("create_dataframe", END)
        
        return workflow.compile()
    
    def generate_pairs(self, state: EvaluationState) -> dict[str, Any]:
        """ç”Ÿæˆæ‰€æœ‰éœ€è¦è¯„ä¼°çš„é…å¯¹ï¼ˆæ¯ä¸ªé…å¯¹è¯„ä¼°æ‰€æœ‰ç»´åº¦ï¼‰"""
        pairs = []
        item_ids = list(state['test_items'].keys())
        
        # åªç”Ÿæˆä¸åŒæ–¹æ³•é—´çš„é¢˜ç›®é…å¯¹ï¼Œæ¯ä¸ªé…å¯¹å°†è¯„ä¼°æ‰€æœ‰ç»´åº¦
        for item1, item2 in combinations(item_ids, 2):
            # æå–æ–¹æ³•ç±»å‹ï¼ˆå‡è®¾IDæ ¼å¼ä¸º"method_index"ï¼‰
            item1_type = item1.split('_')[0]
            item2_type = item2.split('_')[0]
            
            # åªæ·»åŠ ä¸åŒæ–¹æ³•é—´çš„é…å¯¹
            if item1_type != item2_type:
                pairs.append((item1, item2, None))  # Noneè¡¨ç¤ºè¯„ä¼°æ‰€æœ‰ç»´åº¦
        
        # åˆå§‹åŒ–è¿›åº¦æ¡
        progress_bar = None
        if state.get('show_progress', True):
            progress_bar = tqdm(
                total=len(pairs), 
                desc="ğŸ” è¯„ä¼°è¿›åº¦", 
                unit="pair",
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
            )
        
        return {
            'pairs_to_evaluate': pairs,
            'completed_evaluations': [],
            'total_pairs': len(pairs),
            'progress_bar': progress_bar
        }
    
    def batch_evaluations(self, state: EvaluationState) -> dict[str, Any]:
        """å‡†å¤‡ä¸‹ä¸€æ‰¹è¯„ä¼°"""
        remaining_pairs = [
            pair for pair in state['pairs_to_evaluate'] 
            if not any(
                eval.item1_id == pair[0] and eval.item2_id == pair[1]
                for eval in state['completed_evaluations']
            )
        ]
        
        # å–ä¸‹ä¸€æ‰¹
        current_batch = remaining_pairs[:state['batch_size']]
        
        return {
            'current_batch': current_batch,
            'batch_results': []
        }
    
    def process_batch(self, state: EvaluationState) -> dict[str, Any]:
        """å¤„ç†å½“å‰æ‰¹æ¬¡çš„è¯„ä¼°ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        
        async def async_batch_processing():
            semaphore = asyncio.Semaphore(state['max_concurrent'])
            
            async def evaluate_pair_with_progress(item1_id: str, item2_id: str):
                async with semaphore:
                    results, token_usage = await self.evaluate_pair_all_dimensions_async(
                        state['test_items'][item1_id],
                        state['test_items'][item2_id],
                        item1_id,
                        item2_id,
                        state['dimensions']
                    )
                    
                    # æ›´æ–°tokenä½¿ç”¨ç»Ÿè®¡
                    state['token_usage'].add(token_usage.input_tokens, token_usage.output_tokens)
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    if state.get('progress_bar'):
                        state['progress_bar'].update(1)
                        # æ˜¾ç¤ºå½“å‰è¯„ä¼°çš„è¯¦ç»†ä¿¡æ¯
                        current_cost = state['cost_config'].calculate_cost(state['token_usage']) if state.get('cost_config') else 0
                        state['progress_bar'].set_postfix({
                            'é¢˜ç›®': f"{item1_id}-{item2_id}",
                            'ç»´åº¦æ•°': len(results),
                            'è´¹ç”¨': f"${current_cost:.4f}"
                        })
                    
                    return results
            
            # å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡
            tasks = [
                evaluate_pair_with_progress(item1_id, item2_id)
                for item1_id, item2_id, _ in state['current_batch']
            ]
            
            batch_results = await asyncio.gather(*tasks)
            # å±•å¹³ç»“æœåˆ—è¡¨ï¼Œå› ä¸ºæ¯ä¸ªä»»åŠ¡ç°åœ¨è¿”å›å¤šä¸ªè¯„ä¼°ç»“æœ
            flattened_results = []
            for results in batch_results:
                flattened_results.extend(results)
            
            return flattened_results
        
        # æ£€æŸ¥æ˜¯å¦åœ¨Jupyterç¯å¢ƒä¸­å¹¶å¤„ç†äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
            nest_asyncio.apply()
            batch_results = asyncio.run(async_batch_processing())
        except RuntimeError:
            batch_results = asyncio.run(async_batch_processing())
        
        return {
            'batch_results': batch_results,
            'completed_evaluations': batch_results
        }
    
    async def evaluate_pair_all_dimensions_async(
        self,
        item1: dict[str, Any],
        item2: dict[str, Any],
        item1_id: str,
        item2_id: str,
        dimensions: list[dict[str, str]]
    ) -> tuple[list[PairwiseEvaluation], TokenUsage]:
        """å¼‚æ­¥è¯„ä¼°å•ä¸ªé…å¯¹çš„æ‰€æœ‰ç»´åº¦ï¼Œä½¿ç”¨ç»“æ„åŒ–è¾“å‡º"""
        
        # ç¡®ä¿ç»“æ„åŒ–è¾“å‡ºè§£æå™¨å·²è®¾ç½®
        if self.json_parser is None or self.dimension_model is None:
            self.setup_structured_output(dimensions)
        
        prompt = self.create_single_eval(
            item1, item2, 
            dimensions
        )
        
        # åˆ›å»ºå¸¦æœ‰ç»“æ„åŒ–è¾“å‡ºæŒ‡ä»¤çš„LLMé“¾
        format_instructions = self.json_parser.get_format_instructions()
        
        system_content = f"{self.sys_prompt}\n\n{format_instructions}"
        messages = [
            SystemMessage(content=system_content),
            HumanMessage(content=prompt)
        ]
        
        # è®¡ç®—è¾“å…¥tokenæ•°é‡
        input_tokens = self.count_tokens(system_content) + self.count_tokens(prompt)
        
        try:
            # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºé“¾
            chain = self.llm | self.json_parser
            response_dict = await chain.ainvoke(messages)
            response_content = json.dumps(response_dict, ensure_ascii=False)
            
        except Exception as e:
            print(f"âš ï¸  ç»“æ„åŒ–è¾“å‡ºè§£æå¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹å¼: {str(e)[:100]}")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            response = await self.llm.ainvoke(messages)
            response_content = response.content
            response_dict = self._parse_multi_dimension_evaluation_response_fallback(
                response_content, dimensions
            )
        
        # è®¡ç®—è¾“å‡ºtokenæ•°é‡
        output_tokens = self.count_tokens(response_content)
        
        # åˆ›å»ºtokenä½¿ç”¨ç»Ÿè®¡
        token_usage = TokenUsage(
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            total_tokens=input_tokens + output_tokens
        )
        
        # åˆ›å»ºå¤šä¸ªPairwiseEvaluationå¯¹è±¡
        evaluations = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ
        if not response_dict:
            print(f"âš ï¸  é…å¯¹ {item1_id}-{item2_id} çš„æ‰€æœ‰ç»´åº¦è¯„ä¼°å‡å¤±è´¥ï¼Œè·³è¿‡æ­¤é…å¯¹")
            return [], token_usage
        
        for dimension_name, winner in response_dict.items():
            # åªä¸ºæˆåŠŸè§£æçš„ç»´åº¦åˆ›å»ºè¯„ä¼°å¯¹è±¡
            evaluations.append(PairwiseEvaluation(
                item1_id=item1_id,
                item2_id=item2_id,
                dimension=dimension_name,
                winner=winner,
                evaluation_time=datetime.now().isoformat()
            ))
        
        # å¦‚æœåªæœ‰éƒ¨åˆ†ç»´åº¦æˆåŠŸï¼Œç»™å‡ºè­¦å‘Š
        if len(evaluations) < len(dimensions):
            failed_dims = len(dimensions) - len(evaluations)
            print(f"âš ï¸  é…å¯¹ {item1_id}-{item2_id} æœ‰ {failed_dims} ä¸ªç»´åº¦è¯„ä¼°å¤±è´¥")
        
        return evaluations, token_usage
    
    def create_single_eval(
        self,
        item1: dict[str, Any],
        item2: dict[str, Any],
        dimensions: list[dict[str, str]]
    ) -> str:
        """åˆ›å»ºå¤šç»´åº¦è¯„ä¼°æç¤ºè¯ï¼Œä¼˜åŒ–ç»“æ„åŒ–è¾“å‡º"""
        # æ„å»ºç»´åº¦è¯´æ˜
        dimension_descriptions = []
        for i, dim in enumerate(dimensions, 1):
            dimension_descriptions.append(f"{i}. {dim['name']}: {dim['description']}")
        
        # æ„å»ºæœŸæœ›çš„è¾“å‡ºæ ¼å¼è¯´æ˜ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        output_example = {}
        for dim in dimensions:
            output_example[dim['name']] = "A"  # ç¤ºä¾‹å€¼
        
        prompt = f"""è¯·æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªæƒ…æ™¯åˆ¤æ–­æµ‹éªŒé¢˜ç›®åœ¨å¤šä¸ªç»´åº¦ä¸Šçš„è´¨é‡ã€‚

è¯„ä¼°ç»´åº¦ï¼š
{chr(10).join(dimension_descriptions)}

é¢˜ç›®Aï¼š
æƒ…å¢ƒï¼š{item1['situation']}
é€‰é¡¹ï¼š{json.dumps(item1['options'], ensure_ascii=False, indent=2)}

é¢˜ç›®Bï¼š
æƒ…å¢ƒï¼š{item2['situation']}
é€‰é¡¹ï¼š{json.dumps(item2['options'], ensure_ascii=False, indent=2)}

è¯·å¯¹æ¯ä¸ªç»´åº¦åˆ†åˆ«è¯„ä¼°å“ªä¸ªé¢˜ç›®æ›´å¥½ï¼Œé€‰æ‹©"A"æˆ–"B"ã€‚

è¾“å‡ºç¤ºä¾‹æ ¼å¼ï¼š
{json.dumps(output_example, ensure_ascii=False, indent=2)}

æ³¨æ„ï¼šåªèƒ½é€‰æ‹©"A"æˆ–"B"ï¼Œä¸å…è®¸å…¶ä»–å€¼ã€‚"""
        
        return prompt

    def _parse_multi_dimension_evaluation_response_fallback(
        self, 
        response: str, 
        dimensions: list[dict[str, str]]
    ) -> dict[str, str]:
        """å›é€€æ–¹æ¡ˆï¼šè§£æå¤šç»´åº¦LLMè¯„ä¼°å“åº”ï¼ˆä½¿ç”¨åŸå§‹JSONè§£ææ–¹æ³•ï¼‰"""
        try:
            parsed = self._parse_json_with_retry(response)
            
            # éªŒè¯å¹¶æ¸…ç†ç»“æœ
            results = {}
            for dim in dimensions:
                dim_name = dim['name']
                if dim_name in parsed:
                    winner = str(parsed[dim_name]).upper()
                    if winner in ['A', 'B']:
                        results[dim_name] = winner
                    else:
                        # æ— æ³•è§£ææ—¶è·³è¿‡è¯¥ç»´åº¦ï¼Œè€Œä¸æ˜¯è®¾ç½®é»˜è®¤å€¼
                        print(f"âš ï¸  ç»´åº¦ {dim_name} è§£æç»“æœæ— æ•ˆ: {parsed[dim_name]}")
                        continue
                else:
                    print(f"âš ï¸  ç»´åº¦ {dim_name} åœ¨å“åº”ä¸­ç¼ºå¤±")
                    continue
            
            return results
            
        except (json.JSONDecodeError, KeyError) as e:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›ç©ºå­—å…¸
            print(f"âŒ JSONè§£æå®Œå…¨å¤±è´¥: {str(e)}")
            return {}

    def _parse_json_with_retry(self, response: str, max_retries: int = 20) -> dict:
        """å¸¦é‡è¯•æœºåˆ¶çš„JSONè§£æ"""
        import re
        
        for attempt in range(max_retries):
            try:
                # å°è¯•ç›´æ¥è§£æJSON
                if response.strip().startswith('{'):
                    return json.loads(response)
                else:
                    # å¦‚æœä¸æ˜¯çº¯JSONï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                    json_match = re.search(r'\{.*\}', response, re.DOTALL)
                    if json_match:
                        return json.loads(json_match.group())
                    else:
                        raise json.JSONDecodeError("No JSON found", "", 0)
                        
            except (json.JSONDecodeError, KeyError) as e:
                if attempt < max_retries - 1:
                    # è®°å½•é‡è¯•
                    print(f"âš ï¸  JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)[:100]}")
                    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é‡æ–°è°ƒç”¨LLMçš„é€»è¾‘
                    continue
                else:
                    print(f"âŒ JSONè§£ææœ€ç»ˆå¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
                    raise
        
        return {}
    
    def _parse_multi_dimension_evaluation_response(
        self, 
        response: str, 
        dimensions: list[dict[str, str]]
    ) -> dict[str, str]:
        """è§£æå¤šç»´åº¦LLMè¯„ä¼°å“åº”"""
        try:
            parsed = self._parse_json_with_retry(response)
            
            # éªŒè¯å¹¶æ¸…ç†ç»“æœ
            results = {}
            for dim in dimensions:
                dim_name = dim['name']
                if dim_name in parsed:
                    winner = str(parsed[dim_name]).upper()
                    if winner in ['A', 'B']:
                        results[dim_name] = winner
                    else:
                        results[dim_name] = 'None'
                else:
                    results[dim_name] = 'None'
            
            return results
            
        except (json.JSONDecodeError, KeyError):
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä¸ºæ‰€æœ‰ç»´åº¦è¿”å›é»˜è®¤å€¼
            return {dim['name']: 'None' for dim in dimensions}
    
    def should_continue_batching(self, state: EvaluationState) -> str:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­æ‰¹å¤„ç†"""
        remaining_pairs = [
            pair for pair in state['pairs_to_evaluate'] 
            if not any(
                eval.item1_id == pair[0] and eval.item2_id == pair[1]
                for eval in state['completed_evaluations']
            )
        ]
        
        return "continue" if remaining_pairs else "end"
    
    def aggregate_results(self, state: EvaluationState) -> dict[str, Any]:
        """èšåˆè¯„ä¼°ç»“æœ"""
        # å®Œæˆè¿›åº¦æ¡
        if state.get('progress_bar'):
            final_cost = state['cost_config'].calculate_cost(state['token_usage']) if state.get('cost_config') else 0
            state['progress_bar'].set_postfix({
                'çŠ¶æ€': 'å®Œæˆ',
                'æ€»è®¡': f"{len(state['completed_evaluations'])}ä¸ª",
                'æœ€ç»ˆè´¹ç”¨': f"${final_cost:.4f}"
            })
            state['progress_bar'].close()
        
        # æ˜¾ç¤ºå®Œæˆä¿¡æ¯
        total_evaluations = len(state['completed_evaluations'])
        total_dimensions = len({eval.dimension for eval in state['completed_evaluations']})
        total_items = len({
            eval.item1_id for eval in state['completed_evaluations']
        }.union({
            eval.item2_id for eval in state['completed_evaluations']
        }))
        
        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“Š æ€»è®¡: {total_evaluations} ä¸ªé…å¯¹è¯„ä¼°")
        print(f"ğŸ“ ç»´åº¦: {total_dimensions} ä¸ªè¯„ä¼°ç»´åº¦")
        print(f"ğŸ“ é¢˜ç›®: {total_items} ä¸ªä¸åŒé¢˜ç›®")
        
        # æ˜¾ç¤ºtokenä½¿ç”¨å’Œè´¹ç”¨ç»Ÿè®¡
        if state.get('token_usage'):
            token_usage = state['token_usage']
            print(f"\nğŸ’° è´¹ç”¨ç»Ÿè®¡:")
            print(f"   è¾“å…¥tokens: {token_usage.input_tokens:,}")
            print(f"   è¾“å‡ºtokens: {token_usage.output_tokens:,}")
            print(f"   æ€»tokens: {token_usage.total_tokens:,}")
            
            if state.get('cost_config'):
                final_cost = state['cost_config'].calculate_cost(token_usage)
                input_cost = (token_usage.input_tokens / 1_000_000) * state['cost_config'].input_token_rate
                output_cost = (token_usage.output_tokens / 1_000_000) * state['cost_config'].output_token_rate
                print(f"   è¾“å…¥è´¹ç”¨: ${input_cost:.4f}")
                print(f"   è¾“å‡ºè´¹ç”¨: ${output_cost:.4f}")
                print(f"   æ€»è´¹ç”¨: ${final_cost:.4f}")
        
        return {}
    
    def create_dataframe(self, state: EvaluationState) -> dict[str, Any]:
        """åˆ›å»ºæœ€ç»ˆçš„DataFrameç»“æœ"""
        data = [asdict(eval) for eval in state['completed_evaluations']]
        df = pd.DataFrame(data)
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        if len(df) > 0:
            # print(f"\nğŸ“‹ ç»“æœæ‘˜è¦:")
            # print(f"   å¹³å‡ç½®ä¿¡åº¦: {df['confidence'].mean():.3f}")
            # print(f"   ç½®ä¿¡åº¦èŒƒå›´: {df['confidence'].min():.3f} - {df['confidence'].max():.3f}")
            
            # æŒ‰ç»´åº¦æ˜¾ç¤ºèƒœç‡ç»Ÿè®¡
            print(f"\nğŸ† å„ç»´åº¦èƒœç‡ç»Ÿè®¡:")
            for dimension in df['dimension'].unique():
                dim_data = df[df['dimension'] == dimension]
                print(f"   {dimension}:")
                
                # è®¡ç®—æ¯ä¸ªé¢˜ç›®çš„èƒœç‡
                win_counts = {}
                total_counts = {}
                
                for _, row in dim_data.iterrows():
                    winner_id = row['item1_id'] if row['winner'] == 'A' else row['item2_id']
                    loser_id = row['item2_id'] if row['winner'] == 'A' else row['item1_id']
                    
                    win_counts[winner_id] = win_counts.get(winner_id, 0) + 1
                    win_counts[loser_id] = win_counts.get(loser_id, 0)
                    
                    total_counts[row['item1_id']] = total_counts.get(row['item1_id'], 0) + 1
                    total_counts[row['item2_id']] = total_counts.get(row['item2_id'], 0) + 1
                
                # è®¡ç®—å¹¶æ˜¾ç¤ºèƒœç‡
                win_rates = {
                    item_id: win_counts.get(item_id, 0) / total_counts.get(item_id, 1)
                    for item_id in total_counts
                }
                
                sorted_items = sorted(win_rates.items(), key=lambda x: x[1], reverse=True)
                for i, (item_id, rate) in enumerate(sorted_items[:3], 1):
                    print(f"     {i}. é¢˜ç›®{item_id}: {rate:.1%} èƒœç‡")
        df.rename(columns={'item1_id': 'A', 'item2_id': 'B'}, inplace=True)
        
        return {'final_results': df}
            
    def evaluate_test_items(
        self,
        test_items: dict[str, dict[str, Any]],
        dimensions: list[dict[str, str]],
        batch_size: int = 10,
        max_concurrent: int = 5,
        show_progress: bool = True
    ) -> pd.DataFrame:
        """ä¸»è¦çš„è¯„ä¼°å…¥å£å‡½æ•°ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        
        # é¢„ä¼°è´¹ç”¨
        estimated_usage, estimated_cost = self.estimate_cost_for_evaluation(test_items, dimensions)
        
        # æ˜¾ç¤ºå¼€å§‹ä¿¡æ¯
        total_items = len(test_items)
        total_dimensions = len(dimensions)
        total_pairs = len(list(combinations(test_items.keys(), 2)))  # åªè®¡ç®—é…å¯¹æ•°ï¼Œä¸ä¹˜ä»¥ç»´åº¦æ•°
        
        if show_progress:
            print(f"ğŸš€ å¼€å§‹å¿ƒç†æµ‹éªŒé¢˜ç›®è¯„ä¼°")
            print(f"ğŸ“ é¢˜ç›®æ•°é‡: {total_items}")
            print(f"ğŸ“ è¯„ä¼°ç»´åº¦: {total_dimensions}")
            print(f"ğŸ” é…å¯¹æ€»æ•°: {total_pairs}ï¼ˆæ¯å¯¹è¯„ä¼°{total_dimensions}ä¸ªç»´åº¦ï¼‰")
            print(f"âš™ï¸  æ‰¹å¤„ç†å¤§å°: {batch_size}")
            print(f"ğŸ”„ æœ€å¤§å¹¶å‘: {max_concurrent}")
            print(f"ğŸ’¡ ä¼˜åŒ–: å•æ¬¡APIè°ƒç”¨è¯„ä¼°æ‰€æœ‰ç»´åº¦ï¼Œå‡å°‘{(total_dimensions-1)*total_pairs}æ¬¡è°ƒç”¨")
            
            # æ˜¾ç¤ºé¢„ä¼°è´¹ç”¨
            print(f"\nğŸ’° é¢„ä¼°è´¹ç”¨:")
            print(f"   é¢„ä¼°è¾“å…¥tokens: {estimated_usage.input_tokens:,}")
            print(f"   é¢„ä¼°è¾“å‡ºtokens: {estimated_usage.output_tokens:,}")
            print(f"   é¢„ä¼°æ€»tokens: {estimated_usage.total_tokens:,}")
            print(f"   é¢„ä¼°æ€»è´¹ç”¨: ${estimated_cost:.4f}")
            print(f"   è´¹ç‡é…ç½®: è¾“å…¥${self.cost_config.input_token_rate}/1M tokens, è¾“å‡º${self.cost_config.output_token_rate}/1M tokens")
            
            print("-" * 50)
            
            # # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
            # user_input = input("æ˜¯å¦ç»§ç»­æ‰§è¡Œè¯„ä¼°ï¼Ÿ(y/n): ").lower().strip()
            # if user_input not in ['y', 'yes', 'æ˜¯']:
            #     print("è¯„ä¼°å·²å–æ¶ˆ")
            #     return pd.DataFrame()
        
        # åˆ›å»ºåˆå§‹çŠ¶æ€ï¼ˆä½¿ç”¨æ­£ç¡®çš„ç±»å‹ï¼‰
        initial_state = {
            'test_items': test_items,
            'dimensions': dimensions,
            'pairs_to_evaluate': [],
            'completed_evaluations': [],
            'current_batch': [],
            'batch_results': [],
            'final_results': None,
            'batch_size': batch_size,
            'max_concurrent': max_concurrent,
            'show_progress': show_progress,
            'total_pairs': 0,
            'progress_bar': None,
            'cost_config': self.cost_config,
            'token_usage': TokenUsage()
        }
        
        try:
            # åˆ›å»ºå¹¶è¿è¡Œå·¥ä½œæµ
            workflow = self.create_evaluation_workflow()
            final_state = workflow.invoke(initial_state)
            
            # è¿”å›æœ€ç»ˆç»“æœ
            return final_state.get('final_results')
            
        except KeyboardInterrupt:
            # å¤„ç†ç”¨æˆ·ä¸­æ–­
            if initial_state.get('progress_bar'):
                initial_state['progress_bar'].close()
            print("\nâš ï¸  è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
            return pd.DataFrame()
        except Exception as e:
            # å¤„ç†å…¶ä»–å¼‚å¸¸
            if initial_state.get('progress_bar'):
                initial_state['progress_bar'].close()
            print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise